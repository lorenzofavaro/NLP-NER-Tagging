{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "3d8dd84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from conllu import parse_incr, TokenList\n",
    "from enum import Enum\n",
    "from typing import Iterator, List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9fb17e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothingStrategy(Enum):\n",
    "    first = 1\n",
    "\n",
    "\n",
    "smoothing_strategy = None\n",
    "train = open(\"data/train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "test = open(\"data/test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "val = open(\"data/val.conllu\", \"r\", encoding=\"utf-8\")\n",
    "tags = ['START', 'O', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'B-LOC', 'B-MISC', 'B-PER', 'B-ORG', 'END']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a7b393",
   "metadata": {},
   "source": [
    "## Smoothing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a1cbe5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Add the remaining smoothing strategies\n",
    "def unknown_word_emission(smoothing_strategy: Enum) -> float:\n",
    "    if smoothing_strategy == SmoothingStrategy.first:\n",
    "        return 1 / len(tags)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bf04d",
   "metadata": {},
   "source": [
    "## Matrix Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "cb7704b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transition_matrix(tags: List[str], train: Iterator[TokenList]) -> np.array:\n",
    "    transition_matrix = np.zeros((len(tags), len(tags)), dtype=float)\n",
    "\n",
    "    tag_counter = defaultdict(int)\n",
    "    transition_counter = defaultdict(int)\n",
    "\n",
    "    for sentence in parse_incr(train):\n",
    "        # count first tag of sentence and match it with 'START' artificial tag\n",
    "        first_tag = sentence[0]['lemma']\n",
    "        tag_counter['START'] += 1\n",
    "        transition_counter[('START', first_tag)] += 1\n",
    "\n",
    "        # count middle token pairs\n",
    "        for (token_a, token_b) in zip(sentence, sentence[1:]):\n",
    "            tag_counter[token_a['lemma']] += 1\n",
    "            transition_counter[(token_a['lemma'], token_b['lemma'])] += 1\n",
    "\n",
    "        # count last tag of sentence and match it with 'END' artificial tag\n",
    "        last_tag = sentence[-1]['lemma']\n",
    "        tag_counter[last_tag] += 1\n",
    "        transition_counter[(last_tag, 'END')] += 1\n",
    "\n",
    "    for i, t1 in enumerate(tags):\n",
    "        for j, t2 in enumerate(tags):\n",
    "            if tag_counter[t1]:  # if tag occurs at least once\n",
    "                transition_matrix[i][j] = transition_counter[(t1, t2)] / tag_counter[t1]  # compute transition probability\n",
    "\n",
    "    train.seek(0)\n",
    "    return transition_matrix\n",
    "\n",
    "\n",
    "def get_emission_probabilities(train: Iterator[TokenList]) -> Dict[str, str]:\n",
    "    word_tag_counter = defaultdict(int)\n",
    "    tag_counter = defaultdict(int)\n",
    "\n",
    "    for sentence in parse_incr(train):\n",
    "        for token in sentence:\n",
    "            word_tag_counter[(token['form'], token['lemma'])] += 1\n",
    "            tag_counter[token['lemma']] += 1\n",
    "\n",
    "    emission_probabilities = {(word, tag): count / tag_counter[tag] for (word, tag), count in word_tag_counter.items()}  # compute emission probability\n",
    "    return emission_probabilities\n",
    "\n",
    "\n",
    "def get_emission_matrix(tags: List[str], words: List[str], emission_probabilities: [Dict[str, str]]) -> np.array:\n",
    "    emission_matrix = np.zeros((len(tags), len(words)), dtype='float32')\n",
    "    for i, tag in enumerate(tags):\n",
    "        for j, word in enumerate(words):\n",
    "            emission_matrix[i, j] = emission_probabilities.get((word, tag), unknown_word_emission(smoothing_strategy))\n",
    "\n",
    "    return emission_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7705c",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "79396733",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_matrix = get_transition_matrix(tags, train)\n",
    "transition_matrix = transition_matrix[:-1, 1:] # remove last row (END) and first column (START)\n",
    "\n",
    "emission_probabilities = get_emission_probabilities(train)\n",
    "train.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abef94",
   "metadata": {},
   "source": [
    "## Definition of Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "400f10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Need to use logarithm to compute argmax probability in order to improve performance\n",
    "def argmax(viterbi_matrix: np.array, Tm: np.array, Em: np.array, j: int, i: int) -> Tuple[int, float]:\n",
    "    max_prob = 0\n",
    "    max_index = 0\n",
    "    for index, prob in enumerate(viterbi_matrix[:, i-1]):\n",
    "        prob *= Tm[index+1, j] * Em[j, i]\n",
    "        if prob > max_prob:\n",
    "            max_index, max_prob = index, prob\n",
    "    \n",
    "    return max_index, max_prob\n",
    "\n",
    "\n",
    "def viterbi(words: List[str], tags: List[str], Π: np.array, Tm: np.array, Em: np.array) -> List[str]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    words: List[str]\n",
    "        Words sequence of the sentence\n",
    "    tags: List[str]\n",
    "        List of NER tags\n",
    "    Π: np.array\n",
    "        Array of initial probabilities (1, T)\n",
    "    Tm: np.array\n",
    "        Transition matrix (T, T)\n",
    "    Em: np.array\n",
    "        Emission matrix (T, W)\n",
    "    \"\"\"\n",
    "\n",
    "    W = len(words)\n",
    "    T = len(tags)\n",
    "\n",
    "    viterbi_matrix = np.zeros((T, W), dtype=float)\n",
    "    backpointer = np.empty((T, W), dtype=int)\n",
    "\n",
    "    # compute first word initial probability for each tag\n",
    "    viterbi_matrix[:, 0] = [emission * initial_p for emission, initial_p in zip(Em[:, 0], Π)]\n",
    "\n",
    "    # compute probabilities and fill backpointer for the rest of matrix\n",
    "    for i in range(1, W):\n",
    "        for j in range(T):\n",
    "            k, value = argmax(viterbi_matrix, Tm, Em, j, i)\n",
    "            viterbi_matrix[j, i] = value\n",
    "            backpointer[j, i] = k\n",
    "\n",
    "    # get tag index k of last column with highest probability\n",
    "    last_column = list(viterbi_matrix[:, -1])\n",
    "    max_value = max(last_column)\n",
    "    k = last_column.index(max_value)\n",
    "\n",
    "    # get best path walking through backpointer\n",
    "    best_path = list()\n",
    "    for i in range(W-1, -1, -1):\n",
    "        best_path.append(tags[k])\n",
    "        k = backpointer[k, i]\n",
    "    \n",
    "    best_path.reverse()\n",
    "    return best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38094270",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3c0e2733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 267155\n",
      "Correctly tagged: 250487\n",
      "Accuracy: 93.76%\n"
     ]
    }
   ],
   "source": [
    "tag_subset = tags[1:-1] # remove artificial tags START and END\n",
    "\n",
    "total_words = 0\n",
    "correctly_classified = 0\n",
    "for sentence in parse_incr(test):\n",
    "    total_words += len(sentence)\n",
    "    correct_tags = [token['lemma'] for token in sentence]\n",
    "    words = np.array([token['form'] for token in sentence])\n",
    "\n",
    "    emission_matrix = get_emission_matrix(tag_subset, words, emission_probabilities)\n",
    "    Π = transition_matrix[0, :-1]\n",
    "\n",
    "    result_tags = viterbi(words, tag_subset, Π, transition_matrix, emission_matrix)\n",
    "    correctly_classified += sum(x == y for x, y in zip(correct_tags, result_tags))\n",
    "\n",
    "print(f'Total words: {total_words}')\n",
    "print(f'Correctly tagged: {correctly_classified}')\n",
    "print(f'Accuracy: {round(correctly_classified / total_words * 100, 2)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
