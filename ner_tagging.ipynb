{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8dd84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from conllu import parse_incr, TokenList\n",
    "from enum import Enum\n",
    "from typing import Iterator, List, Dict, Tuple\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb17e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Enum):\n",
    "    most_frequent_tag = 1\n",
    "    viterbi = 2\n",
    "\n",
    "class SmoothingStrategy(Enum):\n",
    "    uniform = 1\n",
    "    always_other = 2\n",
    "    other_and_misc = 3\n",
    "    one_shot_word = 4\n",
    "\n",
    "decoder = Decoder.viterbi\n",
    "smoothing_strategy = None\n",
    "laplace_correction = np.finfo(float).tiny\n",
    "lang = 'eng'\n",
    "train_set = open(f'data/{lang}/train.conllu', 'r', encoding='utf-8')\n",
    "test_set = open(f'data/{lang}/test.conllu', 'r', encoding='utf-8')\n",
    "val_set = open(f'data/{lang}/val.conllu', 'r', encoding='utf-8')\n",
    "\n",
    "tags = ['START', 'O', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'B-LOC', 'B-MISC', 'B-PER', 'B-ORG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a7b393",
   "metadata": {},
   "source": [
    "## Smoothing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cbe5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unknown_word_emission(smoothing_strategy: Enum, tag: str) -> float:\n",
    "    if smoothing_strategy == SmoothingStrategy.uniform:\n",
    "        return 1 / len(tags)\n",
    "    elif smoothing_strategy == SmoothingStrategy.always_other:\n",
    "        if tag == 'O':\n",
    "            return 1\n",
    "    elif smoothing_strategy == SmoothingStrategy.other_and_misc:\n",
    "        if tag == 'O' or tag == 'B-MISC':\n",
    "            return 0.5\n",
    "    elif smoothing_strategy == SmoothingStrategy.one_shot_word:\n",
    "        return one_shot_distrib[tag]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585bf04d",
   "metadata": {},
   "source": [
    "## Matrix Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7704b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_matrix(tags: List[str], train_set: Iterator[TokenList]) -> np.array:\n",
    "    transition_matrix = np.zeros((len(tags), len(tags)), dtype=float)\n",
    "\n",
    "    tag_counter = defaultdict(int)\n",
    "    transition_counter = defaultdict(int)\n",
    "\n",
    "    for sentence in parse_incr(train_set):\n",
    "        # count first tag of sentence and match it with 'START' artificial tag\n",
    "        first_tag = sentence[0]['lemma']\n",
    "        tag_counter['START'] += 1\n",
    "        transition_counter[('START', first_tag)] += 1\n",
    "\n",
    "        # count middle token pairs\n",
    "        for (token_a, token_b) in zip(sentence, sentence[1:]):\n",
    "            tag_counter[token_a['lemma']] += 1\n",
    "            transition_counter[(token_a['lemma'], token_b['lemma'])] += 1\n",
    "\n",
    "        # count last tag of sentence\n",
    "        tag_counter[sentence[-1]['lemma']] += 1\n",
    "\n",
    "    for i, t1 in enumerate(tags):\n",
    "        for j, t2 in enumerate(tags):\n",
    "            if tag_counter[t1] > 0:  # if tag occurs at least once\n",
    "                transition_matrix[i, j] = transition_counter[(t1, t2)] / tag_counter[t1]  # compute transition probability\n",
    "\n",
    "    train_set.seek(0)\n",
    "    return transition_matrix\n",
    "\n",
    "\n",
    "def compute_emission_probabilities(train_set: Iterator[TokenList]) -> Dict[str, float]:\n",
    "    word_tag_counter = defaultdict(int)\n",
    "    tag_counter = defaultdict(int)\n",
    "\n",
    "    for sentence in parse_incr(train_set):\n",
    "        for token in sentence:                \n",
    "            word_tag_counter[(token['form'], token['lemma'])] += 1\n",
    "            tag_counter[token['lemma']] += 1\n",
    "    \n",
    "    emission_probabilities = {(word, tag): count / tag_counter[tag] for (word, tag), count in word_tag_counter.items()}  # compute emission probability\n",
    "    train_set.seek(0)\n",
    "    return emission_probabilities\n",
    "\n",
    "\n",
    "def compute_emission_matrix(tags: List[str], words: List[str], emission_probabilities: [Dict[str, float]]) -> np.array:\n",
    "    emission_matrix = np.zeros((len(tags), len(words)), dtype=float)\n",
    "    for i, tag in enumerate(tags):\n",
    "        for j, word in enumerate(words):\n",
    "            emission_matrix[i, j] = emission_probabilities.get((word, tag), unknown_word_emission(smoothing_strategy, tag))\n",
    "\n",
    "    return emission_matrix\n",
    "\n",
    "\n",
    "def compute_one_shot_distrib(val_set: Iterator[TokenList]) -> Dict[str, float]:\n",
    "    word_tag_counter = defaultdict(int)\n",
    "    \n",
    "    for sentence in parse_incr(val_set):\n",
    "        for token in sentence:                \n",
    "            word_tag_counter[(token['form'], token['lemma'])] += 1\n",
    "    \n",
    "    one_shot_distrib = defaultdict(int)\n",
    "    for (word, tag), count in word_tag_counter.items():\n",
    "        if count == 1:\n",
    "            one_shot_distrib[tag] += 1\n",
    "    total_count = sum(one_shot_distrib.values())\n",
    "\n",
    "    for tag, count in one_shot_distrib.items():\n",
    "        one_shot_distrib[tag] = count / total_count\n",
    "    \n",
    "    return one_shot_distrib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7705c",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79396733",
   "metadata": {},
   "outputs": [],
   "source": [
    "emission_probabilities = compute_emission_probabilities(train_set)\n",
    "transition_matrix = np.log(np.add(compute_transition_matrix(tags, train_set), laplace_correction, where=lambda x: not x))\n",
    "\n",
    "if smoothing_strategy == SmoothingStrategy.one_shot_word:\n",
    "    one_shot_distrib = compute_one_shot_distrib(val_set)\n",
    "\n",
    "tags.remove('START')\n",
    "transition_matrix = transition_matrix[1:, 1:]\n",
    "\n",
    "train_set.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91abef94",
   "metadata": {},
   "source": [
    "## Definition of Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(arr: np.array) -> Tuple[int, float]:\n",
    "    max_elem = np.amax(arr)\n",
    "    max_index = np.where(arr == max_elem)[0][0]\n",
    "\n",
    "    return max_index, max_elem\n",
    "\n",
    "\n",
    "def viterbi(words: List[str], tags: List[str], Tm: np.array, Em: np.array) -> List[str]:\n",
    "    W = len(words)\n",
    "    T = len(tags)\n",
    "\n",
    "    viterbi_matrix = np.full((T, W), np.NINF, dtype=float)\n",
    "    backpointer = np.empty((T, W), dtype=int)\n",
    "\n",
    "    # compute first word initial probability for each tag\n",
    "    viterbi_matrix[:, 0] = [emission + initial_p for emission, initial_p in zip(Em[:, 0], Tm[0, :])]\n",
    "\n",
    "    # compute probabilities and fill backpointer for the rest of matrix\n",
    "    for i in range(1, W):\n",
    "        for j in range(T):\n",
    "            k, prob = argmax(viterbi_matrix[:, i-1] + Tm[:, j])\n",
    "            viterbi_matrix[j, i] = prob + Em[j, i]\n",
    "            backpointer[j, i] = k\n",
    "\n",
    "    # get tag index k of last column with highest probability\n",
    "    k, _ = argmax(viterbi_matrix[:, -1])\n",
    "\n",
    "    # get best path walking through backpointer\n",
    "    best_path = list()\n",
    "    for i in range(W-1, -1, -1):\n",
    "        best_path.append(tags[k])\n",
    "        k = backpointer[k, i]\n",
    "    \n",
    "    best_path.reverse()\n",
    "    return best_path\n",
    "\n",
    "\n",
    "def most_frequent_tag(words: List[str], tags: List[str], Em: np.array) -> List[str]:\n",
    "    W = len(words)\n",
    "    prediction = list()\n",
    "    for i in range(W):\n",
    "        k, _ = argmax(Em[:, i])\n",
    "        prediction.append(tags[k])\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38094270",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e2733",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = list()\n",
    "reference = list()\n",
    "\n",
    "for sentence in parse_incr(test_set):\n",
    "    correct_tags = [token['lemma'] for token in sentence]\n",
    "    words = [token['form'] for token in sentence]\n",
    "\n",
    "    emission_matrix = np.log(np.add(compute_emission_matrix(tags, words, emission_probabilities), laplace_correction, where = lambda x: not x))\n",
    "\n",
    "    if decoder == Decoder.viterbi:\n",
    "        prediction = viterbi(words, tags, transition_matrix, emission_matrix)\n",
    "    elif decoder == Decoder.most_frequent_tag:\n",
    "        prediction = most_frequent_tag(words, tags, emission_matrix)\n",
    "    \n",
    "    predictions.extend(prediction)\n",
    "    reference.extend(correct_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3860b3a",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(reference, predictions)\n",
    "report = classification_report(reference, predictions, target_names=tags)\n",
    "confusion = confusion_matrix(reference, predictions, labels=tags)\n",
    "\n",
    "print(f'Total words: {len(predictions)}')\n",
    "print(f'Accuracy: {round(accuracy * 100, 2)}%')\n",
    "print('\\nClassification Report')\n",
    "print(report)\n",
    "print('\\nConfusion Matrix')\n",
    "print(pd.DataFrame(confusion, index=tags, columns=tags))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
